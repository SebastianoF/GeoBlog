{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Employees location respect to their office](images/cover.png \"Employees location respect to their office - Kepler and seaborn\")\n",
    "\n",
    "# How well positioned is your office? \n",
    "\n",
    "### Content\n",
    "\n",
    "1. Problem statement\n",
    "2. Setup and requirements\n",
    "3. Download and visualise the dataset\n",
    "4. Narrow the dataset to the city of London\n",
    "5. Select the office location\n",
    "6. Compute bearing and distance from the selected office to all the commuters residences\n",
    "7. Visualise the results in a radar-histogram plot\n",
    "8. Can we do better?\n",
    "\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "> Is your company office optimally located in respect to the position of its employees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Requirements\n",
    "\n",
    "Conda environment:\n",
    "```\n",
    "conda create -n geods python=3.9\n",
    "conda activate geods\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "```\n",
    "# requirements.txt\n",
    "geopandas==0.10.2\n",
    "jupyter==1.0.0\n",
    "keplergl==0.3.2\n",
    "matplotlib==3.5.1\n",
    "osmnx==1.1.2\n",
    "pandas==1.4.2\n",
    "seaborn==0.11.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and visualise the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import osmnx\n",
    "import pandas as pd\n",
    "\n",
    "from keplergl import KeplerGl\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 10 seconds\n",
    "df_commuter = pd.read_csv(\"https://github.com/uber-web/kepler.gl-data/raw/master/ukcommute/data.csv\")\n",
    "df_commuter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'version': 'v1',\n",
    "    'config': {\n",
    "        'mapState': {\n",
    "            'latitude': 51.536265,\n",
    "            'longitude': -0.039740,\n",
    "            'zoom': 10\n",
    "        }\n",
    "    }\n",
    "}\n",
    "map_1 = KeplerGl(data={'commuters': df_commuter}, config=config, height=800)\n",
    "\n",
    "display(map_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow the dataset to the city of London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osmnx.config(use_cache=True, log_console=True)\n",
    "\n",
    "def gdf_concat(list_gdf: list):\n",
    "    return gpd.GeoDataFrame( pd.concat(list_gdf, ignore_index=True)) \n",
    "\n",
    "query_city = {'city': 'City of London'}\n",
    "query_london = {'city': 'London'}\n",
    "\n",
    "gdf = gdf_concat([osmnx.geocode_to_gdf(query_city), osmnx.geocode_to_gdf(query_london)])\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_epsg = gdf.to_crs(epsg=3857)\n",
    "ax = gdf_epsg.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\n",
    "cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kepler_config import config_map_2\n",
    "except ImportError:\n",
    "    config = config_map_2\n",
    "\n",
    "map_2 = KeplerGl(data={'london' :gdf_epsg}, config=config, height=800)  # kepler knows what to do when fed with a geodataframe\n",
    "display(map_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- about 17 seconds --\n",
    "gdf_commuters_workplace = gpd.GeoDataFrame(df_commuter.copy(), geometry=gpd.points_from_xy(df_commuter.workplace_lng, df_commuter.workplace_lat))\n",
    "\n",
    "# -- about 120 seconds: points in polygon \n",
    "mask_points_in_city = gdf_commuters_workplace.intersects(gdf.geometry.iloc[0])\n",
    "mask_points_in_london = gdf_commuters_workplace.intersects(gdf.geometry.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_rows = len(gdf_commuters_workplace)\n",
    "num_rows_in_city = len(mask_points_in_city[mask_points_in_city == True])\n",
    "num_rows_in_london = len(mask_points_in_london[mask_points_in_london == True])\n",
    "print(f\"Number of rows for offices in the city {num_rows_in_city} ({100 * num_rows_in_city / num_total_rows} %)\")\n",
    "print(f\"Number of rows for offices in london {num_rows_in_london} ({100 * num_rows_in_london / num_total_rows} %)\")\n",
    "\n",
    "mask_union = mask_points_in_city | mask_points_in_london\n",
    "num_rows_in_union = mask_union.sum()\n",
    "print(f\"Number of offices in the union of London and the City {num_rows_in_union} ({100 * num_rows_in_union / num_total_rows} %)\")\n",
    "\n",
    "# Sanity check\n",
    "assert num_rows_in_union == num_rows_in_city + num_rows_in_london\n",
    "\n",
    "df_commuter_london_office = df_commuter[mask_union]\n",
    "df_commuter_london_office.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kepler_config import config_map_3\n",
    "except ImportError:\n",
    "    config_map_3 = config\n",
    "\n",
    "# Use the config_3 in kepler_config.py in the repo to reproduce the same image\n",
    "map_3 = KeplerGl(data={'london':gdf_epsg.copy(),  \"commuters\": df_commuter_london_office.copy()}, config=config_map_3, height=800)\n",
    "display(map_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the office location\n",
    "\n",
    "Geometry drawn by hand on Kepler GL and copy-pasted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_st_luke_office = {\"type\":\"Polygon\",\"coordinates\":[[[-0.0930210043528368,51.52553386809767],[-0.09362754938510826,51.5257442611004],[-0.09398505401347826,51.52546150215205],[-0.09363181940230854,51.525218817282784],[-0.09313761642997592,51.52527679524477],[-0.0930210043528368,51.52553386809767]]]}\n",
    "\n",
    "polygon_albert_road = {\"type\":\"Polygon\",\"coordinates\":[[[0.05074120549614755,51.503014231092195],[0.04882522609357891,51.50189434877025],[0.051410997081145014,51.49996117091324],[0.05337913172491038,51.501678115383754],[0.05074120549614755,51.503014231092195]]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow dataset to the geometry\n",
    "mask_st_luke_office = gdf_commuters_workplace.intersects(shape(polygon_st_luke_office))\n",
    "df_commuters_st_luke_office = df_commuter[mask_st_luke_office]\n",
    "\n",
    "# embed shape into a geopandas to visualise in kepler\n",
    "gdf_st_luke_geometry = gpd.GeoDataFrame({'geometry':[shape(polygon_st_luke_office)], \"display_name\": [\"St Luke's Close Office\"]})\n",
    "\n",
    "\n",
    "# Same for Albert Road office\n",
    "\n",
    "mask_albert_road = gdf_commuters_workplace.intersects(shape(polygon_albert_road))\n",
    "df_commuters_albert_road = df_commuter[mask_albert_road]\n",
    "\n",
    "gdf_albert_road = gpd.GeoDataFrame({'geometry':[shape(polygon_albert_road)], \"display_name\": [\"St Luke's Close Office\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kepler_config import config_map_4\n",
    "except ImportError:\n",
    "    config_map_4 = config\n",
    "\n",
    "map_4 = KeplerGl(\n",
    "    data={\n",
    "        \"St Luke's Close Office\": gdf_st_luke_geometry.copy(),  \n",
    "        \"commuters to St Luke\": df_commuters_st_luke_office.copy(),\n",
    "        \"Albert Road Office\": gdf_albert_road.copy(),\n",
    "        \"commuters to Albert\": df_commuters_albert_road.copy(),\n",
    "    }, \n",
    "    config=config_map_4, \n",
    "    height=800)  # kepler knows what to do when fed with a geodataframe\n",
    "display(map_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Commuters to St Luke office {len(df_commuters_st_luke_office)} ({100 * len(df_commuters_st_luke_office) / len(df_commuter)} %)\" )\n",
    "print(f\"Commuters to Albert Road office {len(df_commuters_albert_road)} ({100 *  len(df_commuters_albert_road) / len(df_commuter)} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bearing and distance of all the commuters to the selected office\n",
    "\n",
    "\n",
    "Given two points $(\\text{lng1}, \\text{lat1})$ and $(\\text{lng2}, \\text{lat2})$ the Haversine formula (geodesic distance on the sphere) is:\n",
    "$$\n",
    "\\mathcal{H} = 2 * R * \\arcsin\\left(\\sqrt{d}~\\right)~,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "d = \\sin^2 \\left(\\frac{\\Delta \\text{lat}}{2} \\right) + \\cos(\\text{lat1}) \\cos(\\text{lat2})  \\sin^2\\left(\\frac{\\Delta \\text{lon}}{2}\\right)~,\n",
    "$$\n",
    "and $\\Delta \\text{lat} = \\text{lat1} - \\text{lat2}$, $\\Delta \\text{lon} = \\text{lon1} - \\text{lon2}$, and $R$ is the hearth's radius.\n",
    "\n",
    "The formula for the bearing, as the angle formed by the geodesics between the north pole and  $(\\text{lng1}, \\text{lat1})$, and the geodesic between  $(\\text{lng1}, \\text{lat1})$ and $(\\text{lng2}, \\text{lat2})$ is  (in radiants):\n",
    "$$\n",
    "\\mathcal{B} = \\arctan\\left( \n",
    "    \\frac{\n",
    "        \\sin(\\Delta \\text{lon}) \\cos(\\text{lat2}) \n",
    "    }{ \n",
    "        \\cos(\\text{lat1}) \\sin(\\text{lat2}) - \\sin(\\text{lat1}) \\cos(\\text{lat2}) \\cos\\left( \\Delta \\text{lon} \\right)\n",
    "    } \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Both formulae are based on the spherical model, not on the geospatial data science the standard model is the ellipsoid model `World Geodesic System 1984 (WGS84)`. Exposing the reason why these formulae above are correct, and generalising them for the WGS84 model (the Vincenty's formulae), would entail expanding the blog post beyond reason. This topic is therefore left as future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from math import radians\n",
    "\n",
    "def haversine(lng1: float, lat1: float, lng2: float, lat2: float) -> Tuple[float, float]:\n",
    "    \"\"\" returns (haversine distance in km, bearing in degrees from point 1 to point 2), vectorised \"\"\"\n",
    "\n",
    "    avg_earth_radius_km = 6371.0072\n",
    "   \n",
    "    lng1, lat1, lng2, lat2 = map(np.deg2rad, [lng1, lat1, lng2, lat2])\n",
    "    d_lat, d_lng = lat2 - lat1, lng2 - lng1\n",
    "    d = np.sin((d_lat)/2)**2 + np.cos(lat1)*np.cos(lat2) * np.sin((d_lng)/2)**2\n",
    "    hav_dist = 2 * avg_earth_radius_km * np.arcsin(np.sqrt(d))\n",
    "   \n",
    "    y = np.sin(d_lng) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(d_lng)\n",
    "    bearing = (np.arctan2(y, x) + 2 * np.pi) % (2 * np.pi)\n",
    "    \n",
    "    return hav_dist, np.rad2deg(bearing)\n",
    "\n",
    "\n",
    "def add_bearing_deg_and_distance_km(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"bearing between A and B is the angle between the geodesics connecting A and the north pole, and the geodesics connecting A and B.\n",
    "    Both the bearing and distance are computed on the Spherical model.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    lng_work, lat_work = df.workplace_lng.to_numpy(), df.workplace_lat.to_numpy()\n",
    "    lng_home, lat_home = df.residence_lng.to_numpy(), df.residence_lat.to_numpy()\n",
    "    \n",
    "    df[\"distance_km\"], df[\"bearing_deg\"] = haversine(lng_work, lat_work, lng_home, lat_home)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commuters_st_luke_office = add_bearing_deg_and_distance_km(df_commuters_st_luke_office)\n",
    "df_commuters_albert_road = add_bearing_deg_and_distance_km(df_commuters_albert_road)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commuters_st_luke_office.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the results in a radar-histogram plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an distance and bearing effective visualisation, a circular histogram would do what we need. The polar visualisation of matplotlib will do this for us.\n",
    "\n",
    "We group the dataset into three categories, according to their radial distance from the office: \n",
    "- Within a radius of 10 Km\n",
    "- Between 10Km and 20 Km\n",
    "- Above 20 Km\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "\n",
    "def radar_histogram(ax, df):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        df with at least 2 columns distance_km and bearing_deg.\n",
    "    Output: radar histogram plot.\n",
    "    \"\"\"\n",
    "    # Figures parameter\n",
    "    directions = 40\n",
    "    \n",
    "    bottom = 4\n",
    "    height_scale = 8\n",
    "    \n",
    "    # bearing: degrees from nort pole clockwise\n",
    "    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n",
    "    # angle visualisation: rad from east counterclockwise\n",
    "    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n",
    "    width = (2*np.pi) / directions\n",
    "    \n",
    "    # data binning\n",
    "    se_bins = pd.cut(df[\"bearing_deg\"].to_numpy(), bearing_bins)\n",
    "    np_bins = se_bins.value_counts().to_numpy()\n",
    "    bins =  height_scale * np.array(np_bins) / np.max(np_bins)\n",
    "    \n",
    "    # Uncomment to debug figure:\n",
    "    # bins = range(directions)\n",
    "    \n",
    "    # plotting    \n",
    "    ax_bars = ax.bar(theta, bins, width=width, bottom=bottom, color=\"blue\")\n",
    "\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n",
    "    ax.grid(False)\n",
    "\n",
    "    return ax\n",
    "\n",
    "def radar_histogram_3_levels(ax, df):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        df with at least 2 columns distance_km and bearing_deg.\n",
    "    Output: radar histogram plot.\n",
    "    \"\"\"\n",
    "    # Figures parameter\n",
    "    directions = 40\n",
    "    height_scale = 2\n",
    "\n",
    "    bottom_inner = 2\n",
    "    bottom_betw = 5\n",
    "    bottom_outer = 8\n",
    "    \n",
    "    # bearing: degrees from nort pole clockwise\n",
    "    bearing_bins = np.linspace(0, 360, directions+1, endpoint=False)\n",
    "    # angle visualisation: rad from east counterclockwise\n",
    "    theta = - 1 * np.linspace(0, 2 * np.pi, directions, endpoint=False) + np.pi/2\n",
    "    width = (2*np.pi) / directions\n",
    "    \n",
    "    # data binning\n",
    "    \n",
    "    df_inner = df[df[\"distance_km\"] <= 10]\n",
    "    se_bins_inner = pd.cut(df_inner[\"bearing_deg\"].to_numpy(), bearing_bins)\n",
    "    np_bins_inner = se_bins_inner.value_counts().to_numpy()\n",
    "    bins_inner =  height_scale * np.array(np_bins_inner) / np.max(np_bins_inner)\n",
    "    \n",
    "    df_betw = df[(df[\"distance_km\"] > 10) & (df[\"distance_km\"] <= 20)]\n",
    "    se_bins_betw = pd.cut(df_betw[\"bearing_deg\"].to_numpy(), bearing_bins)\n",
    "    np_bins_betw = se_bins_betw.value_counts().to_numpy()\n",
    "    bins_betw =  height_scale * np.array(np_bins_betw) / np.max(np_bins_betw)\n",
    "    \n",
    "    df_outer = df[df[\"distance_km\"] > 20]\n",
    "    se_bins_outer = pd.cut(df_outer[\"bearing_deg\"].to_numpy(), bearing_bins)\n",
    "    np_bins_outer = se_bins_outer.value_counts().to_numpy()\n",
    "    bins_outer =  height_scale * np.array(np_bins_outer) / np.max(np_bins_outer)\n",
    "    \n",
    "    # plotting\n",
    "    \n",
    "    ax_bars_inner = ax.bar(theta, bins_inner, width=width, bottom=bottom_inner, color=\"blue\")\n",
    "    ax_bars_betw = ax.bar(theta, bins_betw, width=width, bottom=bottom_betw, color=\"blue\")\n",
    "    ax_bars_outer = ax.bar(theta, bins_outer, width=width, bottom=bottom_outer, color=\"blue\")\n",
    "\n",
    "    \n",
    "    ax.set_yticklabels([])\n",
    "    # uncomment to add values on radius axis\n",
    "    # ax.set_yticks(np.arange(0,10,1.0))\n",
    "    # ax.set_yticklabels(['', '', '', '<=10Km ', '', '', '>10Km\\n <=20Km', '', '', '>20Km'])\n",
    "\n",
    "    ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    ax.set_xticklabels(['E', '', 'N', '', 'W', '', 'S', ''])\n",
    "    ax.grid(False)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "ax11 = fig.add_subplot(221, polar=True)\n",
    "ax11 = radar_histogram(ax11, df_commuters_st_luke_office)\n",
    "ax11.set_title(\"Saint Luke Office\", y=1.08, x=1.1)\n",
    "\n",
    "ax12 = fig.add_subplot(222, polar=True)\n",
    "ax12 = radar_histogram_3_levels(ax12, df_commuters_st_luke_office)\n",
    "\n",
    "\n",
    "ax21 = fig.add_subplot(223, polar=True)\n",
    "ax21 = radar_histogram(ax21, df_commuters_albert_road)\n",
    "ax21.set_title(\"Albert Road Office\", y=1.08, x=1.1)\n",
    "\n",
    "ax22 = fig.add_subplot(224, polar=True)\n",
    "ax22 = radar_histogram_3_levels(ax22, df_commuters_albert_road)\n",
    "\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see that the office in Saint Luke is reasonably well balance across the location of the employees, overall, and splitting the commuters into three categories based on radial distance.\n",
    "\n",
    "The same can not be said for the office located in Albert road office, whose employees should consider to relocate to an office further North-East."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the question \"Is your company office optimally located in respect to the position of its employees?\", we developed a small example of the geospatial data science capabilities to visualise the employees distribution around in respect to the position of their office, via the computation of bearing and distance, to see how off-center it can be, and in which direction it should be relocated to reduce the commuting distance for each employee. In doing so we showed how to download city boundaries from the OSM python API, how to intersect points in polygons, and how visualise geospatial data with Keplerl GL.\n",
    "\n",
    "There are several limitations that are worth mentioning. The first and most obvious one is that the bearing and distance between office and residence is not a the single metric to justify an office relocation. From the point of view of the employee, there are other factors that have not been considered, such as commuting time, cost, frequency of commute, as well as the employee position in the company hierarchy and its \"can't bother\" factor. This last metric, is an empirical one of the will (or lack thereof) to make a change, it is entirely based upon the individual opinion, taking into account for example possible facilities in the new office, traffic, proximity to children's schools and so on. All these parameters has to be considered for the current office location and the potential new office location.\n",
    "\n",
    "From the point of view of the employer, there is of course the cost of the new office, as well as the cost of the move, as well as prestige of location in respect to possible investors.\n",
    "\n",
    "The last limitation is obviously the dataset. The whole post was written around the toy dataset downloaded from the of Kepler GL examples page. Is it realistic enough or reliable? Can we for example make further analysis on the dataset and obtain some statistics and insights about commuters' habits? We already noticed that some, if not all commuting locations coincided with the location of the offices. Can we do some further analysis to know how little we should trust the data? To answer this question, we can end up with some minimal data analysis to the dataset to see if the ratio of number of offices in respect to the number of employees is convincing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_offices = len(df_commuter.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\n",
    "number_of_residences = len(df_commuter.groupby([\"residence_lng\", \"residence_lat\"]).count())\n",
    "\n",
    "number_of_offices_in_london_and_city = len(df_commuter_london_office.groupby([\"workplace_lng\", \"workplace_lat\"]).count())\n",
    "number_of_residences_commuting_to_london_and_city = len(df_commuter_london_office.groupby([\"residence_lng\", \"residence_lat\"]).count())\n",
    "\n",
    "commuters_office_ratio = number_of_residences / number_of_offices\n",
    "commuters_office_ratio_in_london_and_city = number_of_residences_commuting_to_london_and_city / number_of_offices_in_london_and_city\n",
    "\n",
    "print(f\"Number of offices in london and the city {number_of_offices_in_london_and_city} ({number_of_offices_in_london_and_city / number_of_offices} %)\")\n",
    "print(f\"Number of residences commuting to london and the city {number_of_residences_commuting_to_london_and_city} ({number_of_residences_commuting_to_london_and_city / number_of_residences} %)\")\n",
    "print(f\"Number of commuters residences per office {commuters_office_ratio}\")\n",
    "print(f\"Number of commuters residences per office in london and the city {commuters_office_ratio_in_london_and_city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly the ratio of commuter's residences per office can tell us that there is something wrong with the dataset. We could speculated about the fact that the dataset is synthetically generated, or that the arrows direction was swapped when generating the dataset, or both. With no further information, we can only say that no analysis on this dataset can provide us with any reasonable answers or statistics to questions related to commuters in the UK. Nonetheless it is a useful dataset for visualistaion and toy exercises such as the one just presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "- https://geopandas.org/en/stable/index.html\n",
    "- https://stackoverflow.com/questions/65064351/python-osmnx-how-to-download-a-city-district-map-from-openstreetmap-based-on-t\n",
    "- https://gis.stackexchange.com/questions/343725/convert-geojson-to-geopandas-geodataframe\n",
    "- https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "- https://anitagraser.github.io/movingpandas/#:~:text=MovingPandas%20is%20a%20Python%20library,movement%20data%20exploration%20and%20analysis\n",
    "- https://stackoverflow.com/questions/17624310/geopy-calculating-gps-heading-bearing\n",
    "- https://geodesy.noaa.gov/CORS/\n",
    "- https://stackoverflow.com/questions/22562364/circular-polar-histogram-in-python\n",
    "- https://stackoverflow.com/questions/12750355/python-matplotlib-figure-title-overlaps-axes-label-when-using-twiny\n",
    "- https://www.dexplo.org/jupyter_to_medium/\n",
    "\n",
    "## Also source of inspiration for writing this blog post:\n",
    "\n",
    "- [Maxime Labonne](https://mlabonne.github.io/blog/)\n",
    "- [Khuyen Tran](https://khuyentran1476.medium.com/)\n",
    "- [Abdishakur](https://medium.com/@shakasom)\n",
    "- [Herbert Lui](https://herbertlui.medium.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3faa4f1b29183c76bb96688ff1ec00b85234f29e0e0a6e994337c0dbdfcac865"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jupynotes')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
